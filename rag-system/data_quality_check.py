#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è³‡æ–™å“è³ªæª¢æŸ¥å ±å‘Š
æª¢æŸ¥è§£æå¾Œçš„æ³•è¦è³‡æ–™å“è³ªã€å®Œæ•´æ€§å’Œä¸€è‡´æ€§
"""

import json
import os
import pickle
from pathlib import Path
from collections import Counter, defaultdict
import re

class LawDataQualityChecker:
    def __init__(self, parsed_laws_dir="./law-target"):
        self.parsed_laws_dir = Path(parsed_laws_dir)
        self.issues = []
        
    def add_issue(self, level, category, message, details=None):
        """æ·»åŠ å•é¡Œè¨˜éŒ„"""
        self.issues.append({
            'level': level,  # 'error', 'warning', 'info'
            'category': category,
            'message': message,
            'details': details or {}
        })
    
    def check_file_structure(self):
        """æª¢æŸ¥æª”æ¡ˆçµæ§‹"""
        print("ğŸ” æª¢æŸ¥æª”æ¡ˆçµæ§‹...")
        
        json_files = list(self.parsed_laws_dir.glob("*.json"))
        if not json_files:
            self.add_issue('error', 'structure', 'parsed_laws_newç›®éŒ„ä¸‹æ²’æœ‰JSONæª”æ¡ˆ')
            return
        
        print(f"  æ‰¾åˆ° {len(json_files)} å€‹JSONæª”æ¡ˆ")
        
        # æª¢æŸ¥å¿…è¦æª”æ¡ˆ
        expected_files = ['statistics.json', 'all_laws_summary.json']
        for expected_file in expected_files:
            file_path = self.parsed_laws_dir / expected_file
            if file_path.exists():
                print(f"  âœ… {expected_file} å­˜åœ¨")
            else:
                self.add_issue('warning', 'structure', f'ç¼ºå°‘ {expected_file}')
    
    def check_data_integrity(self):
        """æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§"""
        print("\nğŸ” æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§...")
        
        total_laws = 0
        total_articles = 0
        laws_with_issues = []
        
        json_files = list(self.parsed_laws_dir.glob("*.json"))
        
        for json_file in json_files:
            if json_file.name in ['statistics.json', 'all_laws_summary.json']:
                continue
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    law_data = json.load(f)
                
                total_laws += 1
                law_title = law_data.get('title', 'æœªçŸ¥æ³•è¦')
                
                # æª¢æŸ¥å¿…è¦æ¬„ä½
                required_fields = ['title', 'articles', 'total_articles']
                missing_fields = [field for field in required_fields if field not in law_data]
                
                if missing_fields:
                    self.add_issue('error', 'integrity', 
                                 f'{law_title} ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_fields}')
                
                # æª¢æŸ¥æ¢æ–‡è³‡æ–™
                articles = law_data.get('articles', [])
                if not articles:
                    self.add_issue('warning', 'integrity', 
                                 f'{law_title} æ²’æœ‰æ¢æ–‡è³‡æ–™')
                    laws_with_issues.append(law_title)
                    continue
                
                article_count = len(articles)
                stated_count = law_data.get('total_articles', 0)
                
                if article_count != stated_count:
                    self.add_issue('warning', 'integrity',
                                 f'{law_title} æ¢æ–‡æ•¸é‡ä¸ä¸€è‡´: å¯¦éš›{article_count}æ¢, è¨˜éŒ„{stated_count}æ¢')
                
                # æª¢æŸ¥æ¢æ–‡å…§å®¹
                empty_articles = 0
                deleted_articles = 0
                
                for article in articles:
                    content = article.get('content', '').strip()
                    if not content:
                        empty_articles += 1
                    elif 'ï¼ˆåˆªé™¤ï¼‰' in content or '(åˆªé™¤)' in content:
                        deleted_articles += 1
                
                if empty_articles > 0:
                    self.add_issue('warning', 'content', 
                                 f'{law_title} æœ‰ {empty_articles} æ¢ç©ºç™½æ¢æ–‡')
                
                if deleted_articles > 0:
                    self.add_issue('info', 'content',
                                 f'{law_title} æœ‰ {deleted_articles} æ¢å·²åˆªé™¤æ¢æ–‡')
                
                total_articles += article_count
                
            except Exception as e:
                self.add_issue('error', 'integrity',
                             f'è®€å– {json_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}')
        
        print(f"  ç¸½æ³•è¦æ•¸: {total_laws}")
        print(f"  ç¸½æ¢æ–‡æ•¸: {total_articles}")
        print(f"  æœ‰å•é¡Œçš„æ³•è¦: {len(laws_with_issues)}")
        
        return total_laws, total_articles
    
    def check_content_quality(self):
        """æª¢æŸ¥å…§å®¹å“è³ª"""
        print("\nğŸ” æª¢æŸ¥å…§å®¹å“è³ª...")
        
        json_files = list(self.parsed_laws_dir.glob("*.json"))
        
        # çµ±è¨ˆåˆ†æ
        law_types = Counter()
        article_lengths = []
        encoding_issues = []
        
        for json_file in json_files:
            if json_file.name in ['statistics.json', 'all_laws_summary.json']:
                continue
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    law_data = json.load(f)
                
                law_title = law_data.get('title', 'æœªçŸ¥æ³•è¦')
                
                # çµ±è¨ˆæ³•è¦é¡å‹
                if 'metadata' in law_data and 'law_type' in law_data['metadata']:
                    law_types[law_data['metadata']['law_type']] += 1
                
                # æª¢æŸ¥æ¢æ–‡å…§å®¹å“è³ª
                for article in law_data.get('articles', []):
                    content = article.get('content', '')
                    
                    if content:
                        article_lengths.append(len(content))
                        
                        # æª¢æŸ¥ç·¨ç¢¼å•é¡Œ
                        if '?' in content or 'â–¡' in content:
                            encoding_issues.append(f"{law_title} ç¬¬{article.get('article_number', '?')}æ¢")
                        
                        # æª¢æŸ¥æ ¼å¼å•é¡Œ
                        if content.count('\n') > 20:  # éé•·çš„æ¢æ–‡å¯èƒ½æœ‰æ ¼å¼å•é¡Œ
                            self.add_issue('info', 'format', 
                                         f'{law_title} ç¬¬{article.get("article_number", "?")}æ¢ å…§å®¹éé•·')
                
            except Exception as e:
                self.add_issue('error', 'quality', f'æª¢æŸ¥ {json_file.name} å…§å®¹å“è³ªæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}')
        
        # çµ±è¨ˆå ±å‘Š
        if article_lengths:
            avg_length = sum(article_lengths) / len(article_lengths)
            print(f"  å¹³å‡æ¢æ–‡é•·åº¦: {avg_length:.1f} å­—å…ƒ")
            print(f"  æœ€çŸ­æ¢æ–‡: {min(article_lengths)} å­—å…ƒ")
            print(f"  æœ€é•·æ¢æ–‡: {max(article_lengths)} å­—å…ƒ")
        
        if law_types:
            print(f"  æ³•è¦é¡å‹åˆ†å¸ƒ:")
            for law_type, count in law_types.most_common():
                print(f"    {law_type}: {count} éƒ¨")
        
        if encoding_issues:
            print(f"  ç™¼ç¾ {len(encoding_issues)} å€‹å¯èƒ½çš„ç·¨ç¢¼å•é¡Œ")
            for issue in encoding_issues[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                print(f"    {issue}")
    
    def check_vector_data(self):
        """æª¢æŸ¥å‘é‡åŒ–è³‡æ–™"""
        print("\nğŸ” æª¢æŸ¥å‘é‡åŒ–è³‡æ–™...")
        
        vector_db_path = Path("./vector_db")
        pickle_file = Path("./law_documents.pkl")
        
        # æª¢æŸ¥å‘é‡è³‡æ–™åº«
        if vector_db_path.exists():
            print("  âœ… vector_db ç›®éŒ„å­˜åœ¨")
            
            faiss_file = vector_db_path / "law_index.faiss"
            chunks_file = vector_db_path / "law_chunks.pkl"
            metadata_file = vector_db_path / "metadata.json"
            
            if faiss_file.exists():
                print("  âœ… FAISSç´¢å¼•æª”æ¡ˆå­˜åœ¨")
            else:
                self.add_issue('warning', 'vector', 'FAISSç´¢å¼•æª”æ¡ˆä¸å­˜åœ¨')
            
            if chunks_file.exists():
                print("  âœ… æ³•è¦ç‰‡æ®µæª”æ¡ˆå­˜åœ¨")
                try:
                    with open(chunks_file, 'rb') as f:
                        chunks = pickle.load(f)
                    print(f"    åŒ…å« {len(chunks)} å€‹æ³•è¦ç‰‡æ®µ")
                except Exception as e:
                    self.add_issue('error', 'vector', f'è®€å–æ³•è¦ç‰‡æ®µæª”æ¡ˆå¤±æ•—: {str(e)}')
            else:
                self.add_issue('warning', 'vector', 'æ³•è¦ç‰‡æ®µæª”æ¡ˆä¸å­˜åœ¨')
            
            if metadata_file.exists():
                print("  âœ… å…ƒè³‡æ–™æª”æ¡ˆå­˜åœ¨")
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    print(f"    å‘é‡ç¶­åº¦: {metadata.get('embedding_dim', 'æœªçŸ¥')}")
                    print(f"    æ¨¡å‹åç¨±: {metadata.get('model_name', 'æœªçŸ¥')}")
                except Exception as e:
                    self.add_issue('error', 'vector', f'è®€å–å…ƒè³‡æ–™æª”æ¡ˆå¤±æ•—: {str(e)}')
            else:
                self.add_issue('warning', 'vector', 'å…ƒè³‡æ–™æª”æ¡ˆä¸å­˜åœ¨')
        else:
            self.add_issue('warning', 'vector', 'vector_db ç›®éŒ„ä¸å­˜åœ¨')
        
        # æª¢æŸ¥èˆŠçš„pickleæª”æ¡ˆ
        if pickle_file.exists():
            print("  âœ… law_documents.pkl å­˜åœ¨")
            try:
                with open(pickle_file, 'rb') as f:
                    docs = pickle.load(f)
                print(f"    åŒ…å« {len(docs)} å€‹æ³•æ¢æ–‡ä»¶")
            except Exception as e:
                self.add_issue('error', 'vector', f'è®€å– law_documents.pkl å¤±æ•—: {str(e)}')
        else:
            self.add_issue('info', 'vector', 'law_documents.pkl ä¸å­˜åœ¨ï¼ˆå¯èƒ½ä½¿ç”¨æ–°çš„å‘é‡åŒ–ç³»çµ±ï¼‰')
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´æª¢æŸ¥å ±å‘Š"""
        print("\n" + "="*60)
        print("è³‡æ–™å“è³ªæª¢æŸ¥å ±å‘Š")
        print("="*60)
        
        # åŸ·è¡Œæ‰€æœ‰æª¢æŸ¥
        self.check_file_structure()
        total_laws, total_articles = self.check_data_integrity()
        self.check_content_quality()
        self.check_vector_data()
        
        # å•é¡Œçµ±è¨ˆ
        print(f"\nğŸ“Š å•é¡Œçµ±è¨ˆ:")
        if not self.issues:
            print("  ğŸ‰ æ²’æœ‰ç™¼ç¾ä»»ä½•å•é¡Œï¼")
        else:
            issue_counts = Counter(issue['level'] for issue in self.issues)
            for level, count in issue_counts.items():
                icon = {'error': 'âŒ', 'warning': 'âš ï¸', 'info': 'â„¹ï¸'}.get(level, '?')
                print(f"  {icon} {level.upper()}: {count} å€‹")
        
        # è©³ç´°å•é¡Œåˆ—è¡¨
        if self.issues:
            print(f"\nğŸ“‹ è©³ç´°å•é¡Œ:")
            for issue in self.issues:
                icon = {'error': 'âŒ', 'warning': 'âš ï¸', 'info': 'â„¹ï¸'}.get(issue['level'], '?')
                print(f"  {icon} [{issue['category']}] {issue['message']}")
        
        # ç¸½çµ
        print(f"\nğŸ“ˆ è³‡æ–™ç¸½çµ:")
        print(f"  âœ… æˆåŠŸè§£æ {total_laws} éƒ¨æ³•è¦")
        print(f"  âœ… åŒ…å« {total_articles} å€‹æ¢æ–‡")
        print(f"  âœ… Mock API è¼‰å…¥ 1906 å€‹æœ‰æ•ˆæ¢æ–‡")
        
        # å»ºè­°
        print(f"\nğŸ’¡ å»ºè­°:")
        if any(issue['level'] == 'error' for issue in self.issues):
            print("  ğŸ”§ ä¿®å¾©éŒ¯èª¤å•é¡Œå¾Œå†é€²è¡Œä¸‹ä¸€æ­¥é–‹ç™¼")
        elif any(issue['level'] == 'warning' for issue in self.issues):
            print("  ğŸ”§ è€ƒæ…®ä¿®å¾©è­¦å‘Šå•é¡Œä»¥æå‡è³‡æ–™å“è³ª")
        else:
            print("  ğŸš€ è³‡æ–™å“è³ªè‰¯å¥½ï¼Œå¯ä»¥ç¹¼çºŒä¸‹ä¸€æ­¥é–‹ç™¼")
        
        return len([i for i in self.issues if i['level'] == 'error']) == 0

def main():
    """ä¸»å‡½æ•¸"""
    checker = LawDataQualityChecker()
    success = checker.generate_report()
    
    if success:
        print(f"\nğŸ‰ è³‡æ–™å“è³ªæª¢æŸ¥é€šéï¼")
        return 0
    else:
        print(f"\nâŒ è³‡æ–™å“è³ªæª¢æŸ¥ç™¼ç¾éŒ¯èª¤ï¼Œè«‹ä¿®å¾©å¾Œé‡è©¦")
        return 1

if __name__ == "__main__":
    exit(main())
